{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute\n",
    "\n",
    "**Direct Reference** PINN Repository from Jay Roxis\n",
    "\n",
    "**Original Work**: *Maziar Raissi, Paris Perdikaris, and George Em Karniadakis*\n",
    "\n",
    "**Additional Dervative work**: Ben Moseley, PINNs: an introductory crash course\n",
    "\n",
    "**Github Repo** : https://github.com/jayroxis/PINNs/tree/master\n",
    "\n",
    "**Link:** https://github.com/jayroxis/PINNs/blob/master/Burgers%20Equation/Burgers%20Identification%20(PyTorch).ipynb\n",
    "\n",
    "@article{raissi2017physicsI,\n",
    "  title={Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},\n",
    "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
    "  journal={arXiv preprint arXiv:1711.10561},\n",
    "  year={2017}\n",
    "}\n",
    "\n",
    "@article{raissi2017physicsII,\n",
    "  title={Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations},\n",
    "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
    "  journal={arXiv preprint arXiv:1711.10566},\n",
    "  year={2017}\n",
    "}\n",
    "\n",
    "Data gathered from the Ontario Ministry of Health and Statistics Canada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as matplotlib\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import (\n",
    "    linalg,\n",
    "    nn,\n",
    "    Tensor,\n",
    "    stack,\n",
    "    cat,\n",
    "    transpose, \n",
    "    optim,\n",
    "    zeros,\n",
    "    diag,\n",
    "    reshape\n",
    "    )\n",
    "import torch \n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import scipy.io\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "#For Data Generation\n",
    "from scipy import signal\n",
    "from scipy import linalg as linalg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Device Availability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underlying Model and Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mass, stiffness and damping matrices\n",
    "m1 = 20.0\n",
    "m2 = 10.0\n",
    "\n",
    "k1 = 2e3\n",
    "k2 = 1e3\n",
    "k3 = 5e3\n",
    "\n",
    "c1 = 100.0\n",
    "c2 = 110.0\n",
    "c3 = 120.0\n",
    "\n",
    "Mvib = np.asarray([[m1, 0.0], [0.0, m2]], dtype = float)\n",
    "Cvib = np.asarray([[c1+c2, -c2], [-c2, c2+c3]], dtype = float) \n",
    "Kvib = np.asarray([[k1+k2, -k2], [-k2, k2+k3]], dtype = float)\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "# building matrices in continuous time domain\n",
    "n = Mvib.shape[0]\n",
    "I = np.eye(n)\n",
    "Z = np.zeros([n,n])\n",
    "Minv = linalg2.pinv(Mvib)\n",
    "\n",
    "negMinvK = - np.matmul(Minv, Kvib)\n",
    "negMinvC = - np.matmul(Minv, Cvib)\n",
    "\n",
    "Ac = np.hstack((np.vstack((Z,negMinvK)), np.vstack((I,negMinvC))))\n",
    "Bc = np.vstack((Z,Minv))\n",
    "Cc = np.hstack((I,Z))\n",
    "Dc = Z.copy()\n",
    "\n",
    "systemC = (Ac, Bc, Cc, Dc)\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "# building matrices in discrete time domain\n",
    "timetotal = 1\n",
    "t = np.linspace(0,timetotal,1001,dtype = float)\n",
    "dt = t[1] - t[0]\n",
    "\n",
    "sD = signal.cont2discrete(systemC, dt)\n",
    "\n",
    "Ad = sD[0]\n",
    "Bd = sD[1]\n",
    "Cd = sD[2]\n",
    "Dd = sD[3]\n",
    "\n",
    "systemD = (Ad, Bd, Cd, Dd, dt)\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "u = np.zeros((t.shape[0], n))\n",
    "u[:, 0] = 2000*np.ones((t.shape[0],)) # kgmm/s^2\n",
    "u[:, 1] = 3000*np.ones((t.shape[0],)) # kgmm/s^2\n",
    "\n",
    "x0 = np.zeros((Ad.shape[1],), dtype = 'float32')\n",
    "\n",
    "x0[0] = 0.5 # mm\n",
    "x0[1] = 0.0 # mm\n",
    "\n",
    "output = signal.dlsim(systemD, u = u, t = t, x0 = x0)\n",
    "yScipy = output[1]\n",
    "\n",
    "#Multiplicative noise\n",
    "yTarget = yScipy + 1e-1 * np.random.randn(yScipy.shape[0], yScipy.shape[1])\n",
    "\n",
    "#store unpreterbed time\n",
    "t_store = np.reshape(t, (len(t),1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voluntary Data cutoff\n",
    "cutoff = timetotal/4\n",
    "yTarcut = yTarget[0:250,:]\n",
    "t_cut = t_store[0:250,:]\n",
    "u_cut = u[0:250,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_count = 125\n",
    "combined_exact_data = np.hstack((t_cut, u_cut, yTarcut)) #arguments(time,Force,1st and 2nd state) [t_cut u_cut, yTarcut]\n",
    "idxsparse = np.random.choice(range(int(cutoff/dt)), sparse_count, replace = False)\n",
    "idxsparse = np.sort(idxsparse)\n",
    "sparse_exact = combined_exact_data[idxsparse, :]\n",
    "t_train_sparse = sparse_exact[:,0:1]\n",
    "F_train_sparse = sparse_exact[:,1:3]\n",
    "state_train_sparse = sparse_exact[:,3:4] #only observe first state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for generating dense deep network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics Informed Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the physics-guided neural network\n",
    "class PhysicsInformedNN():\n",
    "    def __init__(self, t_data, t_physics, X, layers, F, M, K, x0):\n",
    "        \n",
    "        # Forcing function required to resolve residual\n",
    "        self.F = torch.tensor(F).float().to(device)\n",
    "        self.F = torch.reshape(self.F, (len(F),2,1))\n",
    "\n",
    "        # Store M and K inputs\n",
    "        self.M = M\n",
    "        self.K = K\n",
    "        \n",
    "        # data\n",
    "        self.X = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        self.t_data = torch.tensor(t_data, requires_grad=True).float().to(device)\n",
    "        self.t_physics = torch.tensor(t_physics, requires_grad=True).float().to(device)\n",
    "        \n",
    "        #optional compartment IC\n",
    "        self.x1_init = torch.tensor(x0[0], requires_grad=True).float().to(device)\n",
    "        self.t_init = torch.tensor([[0.]], requires_grad=True).float().to(device)\n",
    "        self.x2_init = torch.tensor(x0[1], requires_grad=True).float().to(device)\n",
    "        self.v1_init = torch.tensor(x0[2], requires_grad=True).float().to(device)\n",
    "        self.v2_init = torch.tensor(x0[3], requires_grad=True).float().to(device)\n",
    "\n",
    "        # initialize unkown model parameter(s)\n",
    "        self.c1_param = torch.nn.Parameter(torch.rand(1, requires_grad=True).to(device))\n",
    "        self.c2_param = torch.nn.Parameter(torch.rand(1, requires_grad=True).to(device))\n",
    "        self.c3_param = torch.nn.Parameter(torch.rand(1, requires_grad=True).to(device))\n",
    "        \n",
    "        # deep neural network\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        self.dnn.register_parameter('c1', self.c1_param)\n",
    "        self.dnn.register_parameter('c2', self.c2_param)\n",
    "        self.dnn.register_parameter('c3', self.c3_param)\n",
    "\n",
    "        # store separate losses for visualization (per epoch)\n",
    "        self.losses = []\n",
    "        self.datalosses = []\n",
    "        self.residlosses = []\n",
    "        self.ICLosses = []\n",
    "        # Track model parameter estimates\n",
    "        self.c1s = []\n",
    "        self.c2s = []\n",
    "        self.c3s = []\n",
    "\n",
    "        # Loss type\n",
    "        self.MSE = torch.nn.MSELoss()\n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters(), lr = 5e-4)\n",
    "        self.iter = 0\n",
    "\n",
    "    # Enforce constraints and apply gains to estimated parameters\n",
    "    @property\n",
    "    def c1(self):\n",
    "        return 20*abs(self.c1_param)\n",
    "\n",
    "    @property\n",
    "    def c2(self):\n",
    "        return 20*abs(self.c2_param)\n",
    "\n",
    "    @property\n",
    "    def c3(self):\n",
    "        return 20*abs(self.c3_param)\n",
    "\n",
    "    def net_x(self, t):  \n",
    "        x = self.dnn(t)\n",
    "        x1 = torch.reshape(x[:,0], (len(t),1))\n",
    "        x2 = torch.reshape(x[:,1], (len(t),1))\n",
    "        return x1, x2\n",
    "    \n",
    "    def net_f(self, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"       \n",
    "        c1 = self.c1\n",
    "        c2 = self.c2\n",
    "        c3 = self.c3\n",
    "        x1net, x2net = self.net_x(t)\n",
    "\n",
    "        x1net_t = torch.autograd.grad(x1net, t, grad_outputs=torch.ones_like(x1net),create_graph=True)[0]\n",
    "        x2net_t = torch.autograd.grad(x2net, t, grad_outputs=torch.ones_like(x2net),create_graph=True)[0]\n",
    "\n",
    "        x1net_tt = torch.autograd.grad(x1net_t, t, grad_outputs=torch.ones_like(x1net_t),create_graph=True)[0]\n",
    "        x2net_tt = torch.autograd.grad(x2net_t, t, grad_outputs=torch.ones_like(x2net_t),create_graph=True)[0]\n",
    "\n",
    "        f1 = self.M[0,0]*x1net_tt +(c1+c2)*x1net_t - c2*x2net_t \\\n",
    "            + self.K[0,0]*x1net + self.K[0,1]*x2net - self.F[:,0,0]\n",
    "        f2 =  self.M[1,1]*x2net_tt +(c1+c3)*x2net_t - c2*x1net_t \\\n",
    "            + self.K[1,1]*x2net + self.K[1,0]*x1net - self.F[:,1,0]\n",
    "        return f1,f2, x1net_t, x2net_t\n",
    "\n",
    "    def train(self, nIter, res_scale, data_scale, IC_scale):\n",
    "        self.dnn.train()\n",
    "        for epoch in range(nIter+1):\n",
    "            x1_pred, x2_pred = self.net_x(self.t_data)\n",
    "            f1, f2, v1net, v2net  = self.net_f(self.t_physics)\n",
    "            #ICs from network\n",
    "            x1_pinit, x2_pinit = self.net_x(self.t_init)\n",
    "            f1init, f2init, v1_pinit, v2_pinit = self.net_f(self.t_init)\n",
    "            loss_data = (self.MSE(x1_pred[:,0], self.X[:,0])) \n",
    "            loss_resid = (torch.mean(torch.square(f1)) + torch.mean(torch.square(f2)))\n",
    "            loss_IC = self.MSE(x1_pinit[0,0], self.x1_init) + self.MSE(x2_pinit[0,0], self.x2_init) \\\n",
    "                    + self.MSE(v1_pinit[0,0], self.v1_init) + self.MSE(v2_pinit[0,0], self.v2_init)\n",
    "            loss = data_scale*loss_data + res_scale*loss_resid + IC_scale*loss_IC\n",
    "\n",
    "            # Backward and optimize\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "            self.c1s.append(self.c1.item())\n",
    "            self.c2s.append(self.c2.item())\n",
    "            self.c3s.append(self.c3.item())\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                self.losses.append(loss.item())\n",
    "                self.residlosses.append(res_scale*loss_resid.item())\n",
    "                self.datalosses.append(data_scale*loss_data.item())\n",
    "                self.ICLosses.append(IC_scale*loss_IC.item())\n",
    "                print(\n",
    "                    'It: %d, Loss: %.3e, c1: %.3f, c2: %.3f, c3: %.3f' % \n",
    "                    (\n",
    "                        epoch, \n",
    "                        loss.item(),\n",
    "                        self.c1.item(),\n",
    "                        self.c2.item(),\n",
    "                        self.c3.item()\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    def predict(self, t):\n",
    "        self.dnn.eval()\n",
    "        #net_x is 'predicted' based off of what is given\n",
    "        x1, x2= self.net_x(t)\n",
    "        x1 = x1.detach().cpu().numpy()\n",
    "        x2 = x2.detach().cpu().numpy()\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network Config\n",
    "layers = [1, 32, 32, 32, 2]\n",
    "# masses, spring coefficients, and damping coefficients\n",
    "m = Tensor([[m1, 0.0], [0.0, m2]]).float().to(device)\n",
    "k = Tensor([[2e3 + 1e3, - 1e3], [- 1e3, 1e3 + 5e3]]).float().to(device)\n",
    "#Physics mesh\n",
    "t_phys = np.linspace(0, 1, 1000)\n",
    "t_phys = np.transpose(t_phys)\n",
    "t_phys = np.reshape(t_phys, (len(t_phys),1))\n",
    "# Requires loading case to be known a priori\n",
    "u_phys = np.zeros((t_phys.shape[0], 2,1))\n",
    "u_phys[:,0,0] = 2000*np.ones((t_phys.shape[0],)) #defines the input force of a step fn for u1 (in kgmm/s^2)\n",
    "u_phys[:,1,0] = 3000*np.ones((t_phys.shape[0],))\n",
    "\n",
    "model = PhysicsInformedNN(t_train_sparse, t_phys, state_train_sparse, layers, u_phys, m, k, x0)\n",
    "\n",
    "#check number of params\n",
    "total_params = sum(p.numel() for p in model.dnn.parameters())\n",
    "print(total_params)\n",
    "\n",
    "model.train(1000, 0.000001, 10.0, 1.0)# (epochs, residuals, data, IC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot true result\n",
    "t_inter = np.linspace(0,1,100)\n",
    "t_inter = np.transpose(t_inter)\n",
    "t_inter = np.reshape(t_inter, (len(t_inter),1))\n",
    "ttensor = torch.tensor(t_inter).float().to(device)\n",
    "[x1net, x2net] = model.predict(ttensor)\n",
    "plt.plot(t_train_sparse, state_train_sparse[:,0], color = 'gray', label = 'Sparse input x1')\n",
    "#plt.plot(t_train_sparse, state_train_sparse[:,1], color = 'gray', label = 'Sparse input x2')\n",
    "plt.plot(t_inter, x1net, color = 'red', label = 'x1 prediction')\n",
    "plt.plot(t_inter, x2net, color = 'red', label = 'x2 prediction')\n",
    "plt.plot(t,yScipy, 'k--', label = 'Truth' )\n",
    "plt.legend()\n",
    "#plt.title('Network recreation of training dataset')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Position [mm]')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Plot Noisy Loss (ADAM)\n",
    "plt.plot(model.losses[0:],color = 'black', label = \"Total\")\n",
    "plt.plot(model.residlosses[0:],color = 'green', label = \"Physics residuals\")\n",
    "plt.plot(model.datalosses[0:],color = 'orange', label = \"Data\")\n",
    "plt.plot(model.ICLosses[0:], color = 'purple', label = \"IC\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Loss trends under ADAM optimization\")\n",
    "plt.xlabel('Epochs (in hundreds)')\n",
    "plt.ylabel('Loss [base 10 log]')\n",
    "plt.show()\n",
    "\n",
    "# Print final loss components\n",
    "print('Adam final losses')\n",
    "print('Loss:  %.4e, Residuals loss: %.4e, Data loss: %.4e, IC loss: %.4e' %\n",
    "        (\n",
    "            model.losses[-1],\n",
    "            model.residlosses[-1],\n",
    "            model.datalosses[-1],\n",
    "            model.ICLosses[-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "print('Estimated c1 value')\n",
    "print(model.c1.item())\n",
    "print('Estimated c2 value')\n",
    "print(model.c2.item())\n",
    "print('Estimated c3 value')\n",
    "print(model.c3.item())\n",
    "\n",
    "# plot the learned c against the true c\n",
    "plt.plot(model.c1s, label = 'c1 PINN estimate', color = 'black')\n",
    "plt.hlines(100, 0, len(model.c1s), label=\"c1 true value\", color=\"black\", linestyle = 'dashed')\n",
    "plt.plot(model.c2s, label = 'c2 PINN estimate', color = 'blue')\n",
    "plt.hlines(110, 0, len(model.c2s), label=\"c3 true value\", color=\"blue\", linestyle = 'dashed')\n",
    "plt.plot(model.c3s, label = 'c3 PINN estimate', color = 'green')\n",
    "plt.hlines(120, 0, len(model.c1s), label=\"c3 true value\", color=\"green\", linestyle = 'dashed')\n",
    "plt.legend()\n",
    "plt.title('Learned damping coefficient over training steps')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Damping coefficient [Ns/m]')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
