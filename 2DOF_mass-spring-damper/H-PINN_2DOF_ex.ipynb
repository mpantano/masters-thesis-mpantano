{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute\n",
    "\n",
    "**Direct Reference**: pinn_ode_tutorial-master github repository\n",
    "\n",
    "**Original Work**: *Renato Nascimento, Kajetan Fricke, Felipe Viana*\n",
    "\n",
    "**Reference Github repo** https://github.com/PML-UCF/pinn_ode_tutorial.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as matplotlib\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch import (\n",
    "    linalg,\n",
    "    nn,\n",
    "    Tensor,\n",
    "    stack,\n",
    "    cat,\n",
    "    transpose, \n",
    "    optim,\n",
    "    zeros,\n",
    "    diag,\n",
    "    reshape,\n",
    "    rand\n",
    "    )\n",
    "\n",
    "#For Data Generation\n",
    "from scipy import signal\n",
    "from scipy import linalg as linalg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underlying Model and Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mass, stiffness and damping matrices\n",
    "m1 = 20.0\n",
    "m2 = 10.0\n",
    "\n",
    "k1 = 2e3\n",
    "k2 = 1e3\n",
    "k3 = 5e3\n",
    "\n",
    "c1 = 100.0\n",
    "c2 = 110.0\n",
    "c3 = 120.0\n",
    "\n",
    "Mvib = np.asarray([[m1, 0.0], [0.0, m2]], dtype = float)\n",
    "Cvib = np.asarray([[c1+c2, -c2], [-c2, c2+c3]], dtype = float) \n",
    "Kvib = np.asarray([[k1+k2, -k2], [-k2, k2+k3]], dtype = float)\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "# building matrices in continuous time domain\n",
    "n = Mvib.shape[0]\n",
    "I = np.eye(n)\n",
    "Z = np.zeros([n,n])\n",
    "Minv = linalg2.pinv(Mvib)\n",
    "\n",
    "negMinvK = - np.matmul(Minv, Kvib)\n",
    "negMinvC = - np.matmul(Minv, Cvib)\n",
    "\n",
    "Ac = np.hstack((np.vstack((Z,negMinvK)), np.vstack((I,negMinvC))))\n",
    "Bc = np.vstack((Z,Minv))\n",
    "Cc = np.hstack((I,Z))\n",
    "Dc = Z.copy()\n",
    "\n",
    "systemC = (Ac, Bc, Cc, Dc)\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "# building matrices in discrete time domain\n",
    "t = np.linspace(0, 1,501,dtype = float)\n",
    "dt = t[1] - t[0]\n",
    "\n",
    "sD = signal.cont2discrete(systemC, dt)\n",
    "\n",
    "Ad = sD[0]\n",
    "Bd = sD[1]\n",
    "Cd = sD[2]\n",
    "Dd = sD[3]\n",
    "\n",
    "systemD = (Ad, Bd, Cd, Dd, dt)\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "u = np.zeros((t.shape[0], n))\n",
    "u[:, 1] = 3*np.ones((t.shape[0],))\n",
    "u[:, 0] = 2*np.ones((t.shape[0],))\n",
    "\n",
    "x0 = np.zeros((Ad.shape[1],), dtype = 'float32')\n",
    "x0[0] = 0.0005\n",
    "\n",
    "output = signal.dlsim(systemD, u = u, t = t, x0 = x0)\n",
    "yScipy = output[1]\n",
    "\n",
    "yTarget = yScipy + 1e-4*np.random.randn(yScipy.shape[0], yScipy.shape[1])\n",
    "\n",
    "#store unpreterbed time\n",
    "t_store = t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid RNN Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct / Deploy Hybrid RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, cell, **kwargs):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.cell = cell\n",
    "\n",
    "    def forward(self, inputs, initial_state):\n",
    "        # Step into proceeding timestep\n",
    "        bs, seq_sz, _ = inputs.shape\n",
    "        state = []\n",
    "        state.append(initial_state)\n",
    "        for t in range(1, seq_sz): \n",
    "            input = inputs[:, t-1, :]\n",
    "            state_t = self.cell.forward(input, state[t-1])\n",
    "            state.append(state[t-1]+state_t)\n",
    "\n",
    "        return stack((state),dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Runge-Kutta Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RungeKuttaIntegratorCell(nn.Module):\n",
    "    def __init__(self, m, c, k, dt, **kwargs):\n",
    "        super(RungeKuttaIntegratorCell, self).__init__(**kwargs)\n",
    "        self.Minv = linalg.inv(diag(m))\n",
    "        self.c1 = Parameter(c[0])\n",
    "        self.c2 = Parameter(c[1])\n",
    "        self.c3 = Parameter(c[2])\n",
    "        \n",
    "        self.K    = Tensor([[k[0]+k[1],-k[1]],[-k[1],k[1]+k[2]]])\n",
    "        # Runge-Kutta iterative vectors\n",
    "        self.state_size    = 2*len(m)\n",
    "        self.A  = Tensor([0., 0.5, 0.5, 1.0])\n",
    "        self.B  = Tensor([[1/6, 2/6, 2/6, 1/6]])\n",
    "        self.dt = dt\n",
    "        \n",
    "    def forward(self, inputs, states):\n",
    "        C = stack((stack((self.c1+self.c2, -self.c2)), stack((-self.c2, self.c2+self.c3))))\n",
    "        y    = states[:, :2]\n",
    "        ydot = states[:, 2:]\n",
    "        #Prepare Runge-Kutta computation\n",
    "        yddoti = self._fun(self.Minv, self.K, C, inputs, y, ydot)\n",
    "        yi     = y + self.A[0] * ydot * self.dt\n",
    "        ydoti  = ydot + self.A[0] * yddoti * self.dt\n",
    "        fn     = self._fun(self.Minv, self.K, C, inputs, yi, ydoti)\n",
    "        #Perfom Runge-Kutta Computation\n",
    "        for j in range(1,4):\n",
    "            yn    = y + self.A[j] * ydot * self.dt\n",
    "            ydotn = ydot + self.A[j] * yddoti * self.dt\n",
    "            ydoti = cat([ydoti, ydotn], dim=0)\n",
    "            fn    = cat([fn, self._fun(self.Minv, self.K, C, inputs, yn, ydotn)], dim=0)\n",
    "\n",
    "        y    = linalg.matmul(self.B, ydoti) * self.dt\n",
    "        ydot =  linalg.matmul(self.B, fn) * self.dt\n",
    "\n",
    "        return cat(([y, ydot]), dim=-1)\n",
    "\n",
    "    def _fun(self, Minv, K, C, u, y, ydot):\n",
    "        return linalg.matmul(u - linalg.matmul(ydot, transpose(C, 0, 1)) - linalg.matmul(y, transpose (K, 0, 1)), transpose(Minv, 0, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinn_training_loop(n_epochs, optimizer, model, loss_fn, train, label, initial_state):\n",
    "    mae = nn.L1Loss()\n",
    "    losses = []\n",
    "    c1s = []\n",
    "    c2s = []\n",
    "    c3s = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        #Forward pass\n",
    "        output_train = model(train, initial_state)[:, :, :2]\n",
    "        loss_train = loss_fn(output_train, label)\n",
    "        mae_train = mae(output_train, label)\n",
    "        #Track loss and c changes\n",
    "        losses.append(loss_train.item())\n",
    "        c1s.append(c[0].item())\n",
    "        c2s.append(c[1].item())\n",
    "        c3s.append(c[2].item())\n",
    "        #Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Training loss {loss_train.item():.4e}, mae {mae_train.item():.4e}\")\n",
    "        \n",
    "    return losses, c1s, c2s, c3s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masses, spring coefficients, and damping coefficients\n",
    "m = Tensor([20.0, 10.0])\n",
    "k = Tensor([2e3, 1e3, 5e3])\n",
    "#c = 1000*rand(3) # initial guess for damping coefficient. Between 0 and 1000\n",
    "c = Tensor([10.0, 10.0, 10.0])\n",
    "\n",
    "# data\n",
    "dt = (t_store[1] - t_store[0])\n",
    "t = Tensor(t_store)\n",
    "utrain = Tensor(u)\n",
    "ytrain = Tensor(yTarget)\n",
    "\n",
    "#resize tensors\n",
    "t = reshape(t,(len(t),1))\n",
    "ufull = reshape(utrain, (1, len(t_store), 2))\n",
    "yfull = reshape(ytrain, (1, len(t_store), 2))\n",
    "utrain = ufull[:,0:126,:]\n",
    "ytrain = yfull[:,0:126,:]\n",
    "t_train = t[0:126,:]\n",
    "\n",
    "# Initial state of the system \n",
    "initial_state = zeros((1,2 * len(m)))\n",
    "initial_state[0,0] = 0.0005\n",
    "\n",
    "rkCell = RungeKuttaIntegratorCell(m=m, c=c, k=k, dt=dt)\n",
    "model = MyRNN(cell=rkCell)\n",
    "    \n",
    "#prediction results before training\n",
    "yPred_before = model(ufull, initial_state)[0, :, :]\n",
    "yPred_before = yPred_before.detach().numpy()[:,:2]\n",
    "\n",
    "#check number of params\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "\n",
    "#PINN training\n",
    "losses, c1s, c2s, c3s = pinn_training_loop(\n",
    "        n_epochs = 900,\n",
    "        optimizer = optim.Adam(model.parameters(), lr=5e2),\n",
    "        model = model,\n",
    "        loss_fn = nn.MSELoss(),\n",
    "        train = utrain,\n",
    "        label = ytrain,\n",
    "        initial_state=initial_state\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction results after training\n",
    "yPred = model(ufull, initial_state) [0, :, :]\n",
    "yPred = yPred.detach().numpy()[:,:2]\n",
    "\n",
    "# plotting prediction results\n",
    "print(c)\n",
    "plt.plot(t[0:126], ytrain[0, :, 0:2], 'gray', label = 'data')\n",
    "plt.plot(t, yPred_before, 'r', label='before training')\n",
    "plt.plot(t, yPred, 'b', label='network output')\n",
    "plt.plot(t, yScipy, 'g', label = 'true value')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Position [m]')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Loss\n",
    "plt.plot(losses,color = 'black')\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Loss trend under Adam optimization\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss [base 10 log]')\n",
    "plt.show()\n",
    "\n",
    "#C track\n",
    "plt.plot(c1s[0:], color = 'black', label = 'c1')\n",
    "plt.axhline(y = 100, color = 'black', linestyle = 'dashed', label = 'c1goal')\n",
    "plt.plot(c2s[0:], color = 'red', label = 'c2')\n",
    "plt.axhline(y = 110, color = 'red', linestyle = 'dashed', label = 'c2goal')\n",
    "plt.plot(c3s[0:], color = 'blue', label = 'c3')\n",
    "plt.axhline(y = 120, color = 'blue', linestyle = 'dashed', label = 'c3goal')\n",
    "plt.legend()\n",
    "plt.title('Change in Parameter estimation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Parameter value')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
