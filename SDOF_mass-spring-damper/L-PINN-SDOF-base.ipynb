{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute\n",
    "\n",
    "**Direct Reference** PINN Repository from Jay Roxis\n",
    "\n",
    "**Original Work**: *Maziar Raissi, Paris Perdikaris, and George Em Karniadakis*\n",
    "\n",
    "**Additional Dervative work**: Ben Moseley, PINNs: an introductory crash course\n",
    "\n",
    "**Github Repo** : https://github.com/maziarraissi/PINNs\n",
    "\n",
    "**Link:** https://github.com/maziarraissi/PINNs/tree/master/appendix/continuous_time_identification%20(Burgers)\n",
    "\n",
    "@article{raissi2017physicsI,\n",
    "  title={Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},\n",
    "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
    "  journal={arXiv preprint arXiv:1711.10561},\n",
    "  year={2017}\n",
    "}\n",
    "\n",
    "@article{raissi2017physicsII,\n",
    "  title={Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations},\n",
    "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
    "  journal={arXiv preprint arXiv:1711.10566},\n",
    "  year={2017}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../Utilities/')\n",
    "\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import warnings\n",
    "import time\n",
    "from scipy import signal\n",
    "from scipy import linalg\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check device availability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for generating dense deep network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics Informed Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the physics-guided neural network\n",
    "class PhysicsInformedNN():\n",
    "    def __init__(self, t_data, t_physics, X, layers, F):\n",
    "        \n",
    "        # Forcing function required to resolve residual\n",
    "        self.Fdata = torch.tensor(F).float().to(device)\n",
    "        self.Fdata = torch.reshape(self.Fdata, (len(F),1))\n",
    "\n",
    "        # data\n",
    "        self.X = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        self.t_data = torch.tensor(t_data, requires_grad=True).float().to(device)\n",
    "        self.t_physics = torch.tensor(t_physics, requires_grad=True).float().to(device)\n",
    "        self.xdata = torch.tensor(self.X[:,0:1], requires_grad=True).float().to(device)\n",
    "        \n",
    "        # initialize unkown model parameter(s)\n",
    "        self.c_param = torch.rand(1, requires_grad=True).to(device)\n",
    "        self.k_param = torch.rand(1, requires_grad=True).to(device)\n",
    "        self.c_param = torch.nn.Parameter(self.c_param)\n",
    "        self.k_param = torch.nn.Parameter(self.k_param)\n",
    "        \n",
    "        # deep neural network\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        self.dnn.register_parameter('c', self.c_param)\n",
    "        self.dnn.register_parameter('k', self.k_param)\n",
    "\n",
    "        # Define coefficients to tune loss component weights\n",
    "        self.res_scale = 0.0001\n",
    "        self.data_scale = 1.0\n",
    "\n",
    "        # store separate losses for visualization (per epoch)\n",
    "        self.losses = []\n",
    "        self.datalosses = []\n",
    "        self.residlosses = []\n",
    "        self.lossesLBFGS = []\n",
    "        self.residlossesLBFGS = []\n",
    "        self.datalossesLBFGS = []\n",
    "        self.cees = [] #track progress of 'c'\n",
    "        self.kays = [] #track progress of 'k'\n",
    "\n",
    "\n",
    "         # optimizers: using the same settings\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=50000, \n",
    "            max_eval=50000, \n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-5, \n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
    "        )\n",
    "        \n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters(), lr = 1e-3)\n",
    "        self.iter = 0\n",
    "\n",
    "     #force parameters to be in a range\n",
    "    @property\n",
    "    def c(self):\n",
    "        return 30*abs(self.c_param)\n",
    "\n",
    "    @property\n",
    "    def k(self):\n",
    "        return 30*abs(self.k_param)\n",
    "\n",
    "\n",
    "    def net_x(self, t):  \n",
    "        x = self.dnn(t)\n",
    "        return x\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"       \n",
    "        c = self.c\n",
    "        k = self.k\n",
    "        xnet = self.net_x(t)\n",
    "\n",
    "        xnet_t = torch.autograd.grad(\n",
    "            xnet, t, \n",
    "            grad_outputs=torch.ones_like(xnet),\n",
    "            create_graph=True,\n",
    "        )[0]\n",
    "        xnet_tt = torch.autograd.grad(\n",
    "            xnet_t, t, \n",
    "            grad_outputs=torch.ones_like(xnet_t),\n",
    "            create_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        f = 1.0*xnet_tt + c*xnet_t + k*xnet # - self.Fdata (assumed to be zero usually) \n",
    "        return f\n",
    "    \n",
    "    def loss_func(self):         \n",
    "        x_pred = self.net_x(self.t_data)\n",
    "        f_pred = self.net_f(self.xdata, self.t_physics)\n",
    "        loss_data =  torch.mean((self.xdata - x_pred) ** 2)\n",
    "        loss_resid = torch.mean(f_pred ** 2)\n",
    "        loss = self.res_scale*loss_resid + self.data_scale*loss_data\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.cees.append(self.c.item())\n",
    "        self.kays.append(self.k.item())\n",
    "\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            self.lossesLBFGS.append(loss.item())\n",
    "            self.residlossesLBFGS.append(self.res_scale*loss_resid.item())\n",
    "            self.datalossesLBFGS.append(self.data_scale*loss_data.item())\n",
    "            print(\n",
    "                'Loss: %e, c: %.3e,  k: %.3f' % \n",
    "                (\n",
    "                    loss.item(), \n",
    "                    self.c.item(),\n",
    "                    self.k.item()\n",
    "                )\n",
    "            )\n",
    "        return loss\n",
    "    \n",
    "    def train(self, nIter):\n",
    "        self.dnn.train()\n",
    "        for epoch in range(nIter+1):\n",
    "            x_pred = self.net_x(self.t_data)\n",
    "            f_pred = self.net_f(self.xdata, self.t_physics)\n",
    "            loss_data = torch.mean((self.xdata - x_pred) ** 2)\n",
    "            loss_resid = torch.mean(f_pred ** 2)\n",
    "            loss = self.res_scale*loss_resid + self.data_scale*loss_data\n",
    "\n",
    "            # Backward and optimize\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "            self.cees.append(self.c.item())\n",
    "            self.kays.append(self.k.item())\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                self.losses.append(loss.item())\n",
    "                self.residlosses.append(self.res_scale*loss_resid.item())\n",
    "                self.datalosses.append(self.data_scale*loss_data.item())\n",
    "                print(\n",
    "                    'It: %d, Loss: %.3e, c: %.3e, k: %.3f' % \n",
    "                    (\n",
    "                        epoch, \n",
    "                        loss.item(),\n",
    "                        self.c.item(),\n",
    "                        self.k.item()\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "    def predict(self, t):\n",
    "        self.dnn.eval()\n",
    "        #net_x is 'predicted' based off of what is given\n",
    "        x = self.net_x(t)\n",
    "        #net_f is always performed under the training scheme\n",
    "        f = self.net_f(self.xdata, self.t_physics)\n",
    "        x = x.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return x, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statespace Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a SDOF MSDS using a statespace function\n",
    "def statespacegenerator(m,c,k,T,dt,x0):\n",
    "    #System input force in Newtons\n",
    "    F_train = np.zeros((int(T/dt)))\n",
    "    j = 0\n",
    "    for i in range (0,int(T/dt)):\n",
    "        F_train[[i]] = 0\n",
    "        j = j+dt\n",
    "\n",
    "    #State space model for a SDOF system\n",
    "    #discretize the time steps \n",
    "    tdiscrete = np.arange(0,T,dt)\n",
    "\n",
    "    A = [[0,1], [-k/m, -c/m]]\n",
    "    B = [[0],[1/m]]\n",
    "    C = [[1,0]]\n",
    "    SDOFsys = signal.StateSpace(A,B,C,0)\n",
    "    [timetrace, y, x] = signal.lsim(SDOFsys, F_train, tdiscrete, x0)\n",
    "\n",
    "    return timetrace, y, x, F_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthesize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1, 32, 32, 32, 1]\n",
    "#Generate synthetic data\n",
    "x0 = np.array([1,0])\n",
    "timestep = 0.001\n",
    "m = 1\n",
    "c = 4\n",
    "k = 400\n",
    "timetotal = 1\n",
    "[t,y,x,F] = statespacegenerator(m,c,k,timetotal,timestep,x0)\n",
    "#Re-define 'xtrain' as the network input\n",
    "t_train = np.transpose(t)\n",
    "t_train = np.reshape(t_train, (len(t),1))\n",
    "F_train = np.transpose(F)\n",
    "F_train = np.reshape(F_train, (len(F),1))\n",
    "state_train = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t,x[:,0])\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Displacement of x1 [m]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparsify time and x outputs\n",
    "sparse_count = 30\n",
    "combined_exact_data = np.hstack((t_train, F_train, state_train))\n",
    "#combined_exact_data = np.hstack((t_train, F_train, state_train))\n",
    "idxsparse = np.random.choice(range(int(indextrain)), sparse_count, replace = False)\n",
    "idxsparse = np.sort(idxsparse)\n",
    "sparse_exact = combined_exact_data[idxsparse, :]\n",
    "t_train_sparse = sparse_exact[:,0:1]\n",
    "F_train_sparse = sparse_exact[:,1:2]\n",
    "state_train_sparse = sparse_exact[:,2:4]\n",
    "y_sparse = state_train_sparse[:,0:1]\n",
    "\n",
    "\n",
    "#noisy option\n",
    "noise = np.random.normal(0,1, state_train_sparse.shape)\n",
    "state_train_sparse_noise = state_train_sparse + (0.1)*noise#*state_train_sparse\n",
    "y_sparse_noise = state_train_sparse_noise[:,0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train with sparse data\n",
    "# line up physics mesh to align with baseline\n",
    "t_phys = np.linspace(0,1,1000)\n",
    "t_phys = np.transpose(t_phys)\n",
    "t_phys = np.reshape(t_phys, (len(t_phys),1))\n",
    "model = PhysicsInformedNN(t_train_sparse, t_phys, state_train_sparse_noise, layers, F_train)\n",
    "\n",
    "#check number of params\n",
    "total_params = sum(p.numel() for p in model.dnn.parameters())\n",
    "print(total_params)\n",
    "\n",
    "model.train(35000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Data visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print noisy ICs\n",
    "#print(x0noisy)\n",
    "\n",
    "#plot true result\n",
    "t_inter = np.linspace(0,1,100)\n",
    "t_inter = np.transpose(t_inter)\n",
    "t_inter = np.reshape(t_inter, (len(t_inter),1))\n",
    "ttensor = torch.tensor(t_inter).float().to(device)\n",
    "[ynet, fnet] = model.predict(ttensor)\n",
    "plt.plot(t_inter, ynet, color = 'red', label = 'Network prediction')\n",
    "plt.scatter(t_train_sparse, y_sparse_noise, label = 'Sparse input')\n",
    "plt.plot(t,y, 'k--', label = 'Truth' )\n",
    "plt.legend()\n",
    "#plt.title('Network recreation of training dataset')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Position [m]')\n",
    "plt.show()\n",
    "\n",
    "#Plot residuals\n",
    "plt.plot(t_phys, fnet, color = 'purple')\n",
    "#plt.title('Physics Residuals from Trained Network')\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Residuals [N]')\n",
    "plt.show()\n",
    "\n",
    "#Plot Noisy Loss (ADAM)\n",
    "plt.plot(model.losses[0:],color = 'black', label = \"Total\")\n",
    "plt.plot(model.residlosses[0:],color = 'green', label = \"Physics residuals\")\n",
    "plt.plot(model.datalosses[0:],color = 'orange', label = \"Data\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "#plt.title(\"Loss trends under ADAM optimization\")\n",
    "plt.xlabel('Epochs (in hundreds)')\n",
    "plt.ylabel('Loss [log 10 base]')\n",
    "plt.show()\n",
    "# Print final loss components\n",
    "print('Adam final losses')\n",
    "print('Loss:  %.4e, Residuals loss: %.4e, Data loss: %.4e' % \n",
    "        (\n",
    "            model.losses[-1],\n",
    "            model.residlosses[-1],\n",
    "            model.datalosses[-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "print('Estimated c value')\n",
    "print(model.c.item())\n",
    "\n",
    "print('Estimated k value')\n",
    "print(model.k.item())\n",
    "\n",
    "# plot the learned c against the true c\n",
    "plt.plot(model.cees, label = 'c', color=\"red\")\n",
    "plt.hlines(4, 0, len(model.cees), label=\"c true value\", color=\"red\", linestyle = 'dashed')\n",
    "plt.legend()\n",
    "#plt.title('Learned damping coefficient over training steps')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('c Parameter estimate')\n",
    "plt.show()\n",
    "\n",
    "# learned k against true k\n",
    "plt.plot(model.kays, label = 'k', color=\"green\")\n",
    "plt.hlines(400, 0, len(model.kays), label=\"k true value\", color=\"green\", linestyle = 'dashed')\n",
    "plt.legend()\n",
    "#plt.title('Learned damping coefficient over training steps')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('k parameter estimate')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
