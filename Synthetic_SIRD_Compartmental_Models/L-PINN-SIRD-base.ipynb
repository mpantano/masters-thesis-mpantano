{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribute\n",
    "\n",
    "**Direct Reference** PINN Repository from Jay Roxis\n",
    "\n",
    "**Original Work**: *Maziar Raissi, Paris Perdikaris, and George Em Karniadakis*\n",
    "\n",
    "**Additional Dervative work**: Ben Moseley, PINNs: an introductory crash course\n",
    "\n",
    "**Github Repo** : https://github.com/jayroxis/PINNs/tree/master\n",
    "\n",
    "**Link:** https://github.com/jayroxis/PINNs/blob/master/Burgers%20Equation/Burgers%20Identification%20(PyTorch).ipynb\n",
    "\n",
    "@article{raissi2017physicsI,\n",
    "  title={Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},\n",
    "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
    "  journal={arXiv preprint arXiv:1711.10561},\n",
    "  year={2017}\n",
    "}\n",
    "\n",
    "@article{raissi2017physicsII,\n",
    "  title={Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations},\n",
    "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
    "  journal={arXiv preprint arXiv:1711.10566},\n",
    "  year={2017}\n",
    "}\n",
    "\n",
    "Data gathered from the Ontario Ministry of Health and Statistics Canada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../Utilities/')\n",
    "\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import warnings\n",
    "import time\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check device availability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA support \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for generating dense deep network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.ReLU\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics Informed Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the physics-guided neural network\n",
    "class PhysicsInformedNN():\n",
    "    def __init__(self, t_data, t_physics, X, N, layers):\n",
    "        \n",
    "        # data\n",
    "        self.X = torch.tensor(X, requires_grad=True).float().to(device)\n",
    "        self.S, self.I, self.D, self.R = self.X\n",
    "        self.t_data = torch.tensor(t_data, requires_grad=True).float().to(device)\n",
    "        self.t_physics = torch.tensor(t_physics, requires_grad=True).float().to(device)\n",
    "        self.N = N\n",
    "        \n",
    "        # initialize unkown model parameter(s)\n",
    "        self.beta_param = torch.nn.Parameter(torch.rand(1, requires_grad=True).to(device))\n",
    "        self.gamma_param = torch.nn.Parameter(torch.rand(1, requires_grad=True).to(device))\n",
    "        self.mu_param = torch.nn.Parameter(torch.rand(1, requires_grad=True).to(device))\n",
    "        \n",
    "\n",
    "        # deep neural network\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        self.dnn.register_parameter('gamma', self.gamma_param)\n",
    "        self.dnn.register_parameter('beta', self.beta_param)\n",
    "        self.dnn.register_parameter('mu', self.mu_param)\n",
    "\n",
    "        # Define coefficients to tune loss component weights\n",
    "        self.res_comp_scale = 100.0\n",
    "        self.res_pop_scale = 0.0001\n",
    "        self.data_scale = 1.0\n",
    "\n",
    "        # store separate losses for visualization (per epoch)\n",
    "        self.losses = []\n",
    "        self.datalosses = []\n",
    "        self.residlosses = []\n",
    "        self.Nlosses = []\n",
    "        self.betas = [] #track progress of 'beta'\n",
    "        self.gammas = [] #track progress of 'gamma'\n",
    "        self.mus = [] #track progress of 'mu'\n",
    "\n",
    "         # optimizers: using the same settings\n",
    "        l_r = 1e-6\n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters(), lr = l_r)\n",
    "        self.iter = 0\n",
    "        self.adam_schedule = torch.optim.lr_scheduler.CyclicLR(self.optimizer_Adam, base_lr=1e-5, \\\n",
    "            max_lr=1e-3, step_size_up=1000, mode=\"exp_range\", gamma=0.85, cycle_momentum=False)\n",
    "        \n",
    "    #force parameters to be in a range\n",
    "    @property\n",
    "    def beta(self):\n",
    "        return torch.tanh(abs(self.beta_param))\n",
    "\n",
    "    @property\n",
    "    def gamma(self):\n",
    "        return torch.tanh(abs(self.gamma_param))\n",
    "\n",
    "    @property\n",
    "    def mu(self):\n",
    "        return torch.tanh(abs(self.mu_param))\n",
    "    \n",
    "    def net_x(self, t):  \n",
    "        out = self.dnn(t)\n",
    "        S = torch.reshape(out[:,0], (len(t), 1))\n",
    "        I = torch.reshape(out[:,1], (len(t), 1))\n",
    "        D = torch.reshape(out[:,2], (len(t), 1))\n",
    "        R = torch.reshape(out[:,3], (len(t), 1))\n",
    "        return S, I, D, R\n",
    "    \n",
    "    def net_f(self, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"       \n",
    "        snet, inet, dnet, rnet = self.net_x(t)\n",
    "\n",
    "        snet_t = torch.autograd.grad(\n",
    "            snet, t, \n",
    "            grad_outputs=torch.ones_like(snet),\n",
    "            create_graph=True,\n",
    "        )[0]\n",
    "        inet_t = torch.autograd.grad(\n",
    "            inet, t, \n",
    "            grad_outputs=torch.ones_like(inet),\n",
    "            create_graph=True,\n",
    "        )[0]\n",
    "        dnet_t = torch.autograd.grad(\n",
    "            dnet, t, \n",
    "            grad_outputs=torch.ones_like(dnet),\n",
    "            create_graph=True,\n",
    "        )[0]\n",
    "        rnet_t = torch.autograd.grad(\n",
    "            rnet, t, \n",
    "            grad_outputs=torch.ones_like(rnet),\n",
    "            create_graph=True,\n",
    "        )[0]\n",
    "\n",
    "        s_res = snet_t + (self.beta / self.N)*snet*inet\n",
    "        i_res = inet_t -  (self.beta / self.N)*snet*inet + self.gamma*inet + self.mu*inet\n",
    "        d_res = dnet_t - self.mu*inet\n",
    "        r_res = rnet_t - self.gamma*inet\n",
    "        N_res = self.N - snet - inet - dnet - rnet\n",
    "        return s_res, i_res, d_res, r_res, N_res\n",
    "    \n",
    "    def train(self, nIter):\n",
    "        self.dnn.train()\n",
    "        for epoch in range(nIter+1):\n",
    "            s_pred, i_pred, d_pred, r_pred = self.net_x(self.t_data)\n",
    "            s_res_pred, i_res_pred, d_res_pred, r_res_pred, N_res_pred = self.net_f(self.t_physics)\n",
    "\n",
    "            loss_data =  (torch.mean(torch.square(self.S - s_pred)) +\n",
    "                        torch.mean(torch.square(self.I - i_pred)) + \n",
    "                        torch.mean(torch.square(self.D - d_pred)) +\n",
    "                        torch.mean(torch.square(self.R - r_pred)))\n",
    "            loss_resid = (torch.mean(torch.square(s_res_pred)) +\n",
    "                        torch.mean(torch.square(i_res_pred)) + \n",
    "                        torch.mean(torch.square(d_res_pred)) +\n",
    "                        torch.mean(torch.square(r_res_pred)))\n",
    "            loss_N =  torch.mean(torch.square(N_res_pred))\n",
    "            loss = self.res_comp_scale*loss_resid + self.data_scale*loss_data + self.res_pop_scale*loss_N\n",
    "\n",
    "            # Backward and optimize\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "            self.adam_schedule.step()\n",
    "            self.betas.append(self.beta.item())\n",
    "            self.gammas.append(self.gamma.item())\n",
    "            self.mus.append(self.mu.item())\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                self.losses.append(loss.item())\n",
    "                self.residlosses.append(self.res_comp_scale*loss_resid.item())\n",
    "                self.datalosses.append(self.data_scale*loss_data.item())\n",
    "                self.Nlosses.append(self.res_pop_scale*loss_N.item())\n",
    "                print(\n",
    "                    'It: %d, Loss: %.5f, beta: %.5f, gamma: %.3f, mu: %.3f ' % \n",
    "                    (\n",
    "                        epoch, \n",
    "                        loss.item(),\n",
    "                        self.beta.item(),\n",
    "                        self.gamma.item(),\n",
    "                        self.mu.item()\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    def predict(self, t):\n",
    "        self.dnn.eval()\n",
    "        #net_x is 'predicted' based off of what is given\n",
    "        s, i, d, r = self.net_x(t)\n",
    "        s = s.detach().cpu().numpy()\n",
    "        i = i.detach().cpu().numpy()\n",
    "        d = d.detach().cpu().numpy()\n",
    "        r = r.detach().cpu().numpy()\n",
    "\n",
    "        return s, i, d, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the differentiations of the compartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compartmentaldiff(y, t, beta, gamma, mu):\n",
    "    S, I, D, R = y\n",
    "    Sdot = - (beta / N) * S * I\n",
    "    Idot = (beta / N) * S * I - gamma * I - mu * I\n",
    "    Rdot = gamma * I\n",
    "    Ddot = mu * I\n",
    "\n",
    "    return Sdot, Idot, Ddot, Rdot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial conditions\n",
    "Ntrue = 59e6 #true population size\n",
    "N = 1.0 #population size squished into 1 for ease of training\n",
    "S0 = (Ntrue - 1)/Ntrue\n",
    "I0 = N - S0\n",
    "R0 = 0\n",
    "D0 = 0\n",
    "y0 = S0, I0, D0, R0\n",
    "\n",
    "# A grid of time points (in days)\n",
    "timesteps = 500\n",
    "duration = 500\n",
    "t = np.linspace(0, duration, timesteps) #from day 0 to day 500, generate 100 points\n",
    "\n",
    "# compartmental model parameters\n",
    "beta = 0.191\n",
    "gamma = 0.05\n",
    "mu = 0.0294\n",
    "\n",
    "# Integrate the SIR equations over the time grid, t.\n",
    "ret = odeint(compartmentaldiff, y0, t, args=(beta, gamma, mu))\n",
    "S, I, D, R = ret.T\n",
    "S = np.reshape(S, (len(t), 1))\n",
    "I = np.reshape(I, (len(t), 1))\n",
    "D = np.reshape(D, (len(t), 1))\n",
    "R = np.reshape(R, (len(t), 1))\n",
    "\n",
    "\n",
    "#reshape time vector\n",
    "t_data = np.transpose(t)\n",
    "t_data = np.reshape(t_data, (len(t),1))\n",
    "\n",
    "# keep clean data\n",
    "Sc = S\n",
    "Ic = I\n",
    "Dc = D\n",
    "Rc = R\n",
    "\n",
    "#relative noise\n",
    "noise = 0.05\n",
    "S = (1 + noise * np.random.normal(0,1, S.shape)) * S\n",
    "I = (1 + noise * np.random.normal(0,1, I.shape)) * I\n",
    "D = (1 + noise * np.random.normal(0,1, D.shape)) * D\n",
    "R = (1 + noise * np.random.normal(0,1, R.shape)) * R\n",
    "\n",
    "# package all noisy data\n",
    "Y = S, I, D, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voluntarily Sparsify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparsify time and x outputs\n",
    "sparse_count = 15\n",
    "t_train = 500\n",
    "steps = 500\n",
    "combined_exact_data = np.hstack((t_data[0:t_train], S[0:t_train], I[0:t_train], D[0:t_train], R[0:t_train])) #artificially cutoff time domain early\n",
    "idxsparse = np.random.choice(range(steps), sparse_count, replace = False)\n",
    "idxsparse = np.sort(idxsparse)\n",
    "sparse_exact = combined_exact_data[idxsparse, :]\n",
    "t_data_sparse = sparse_exact[:,0:1]\n",
    "S_sparse, I_sparse, D_sparse, R_sparse = sparse_exact[:,1:2], sparse_exact[:,2:3], sparse_exact[:,3:4], sparse_exact[:,4:5] \n",
    "Y_sparse = S_sparse, I_sparse, D_sparse, R_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)\n",
    "ax.set_facecolor('xkcd:white')\n",
    "\n",
    "ax.plot(t_data_sparse, S_sparse, 'violet', alpha=0.5, lw=2, label='Susceptible', linestyle='dashed')\n",
    "ax.plot(t_data_sparse, I_sparse, 'darkgreen', alpha=0.5, lw=2, label='Infected', linestyle='dashed')\n",
    "ax.plot(t_data_sparse, D_sparse, 'blue', alpha=0.5, lw=2, label='Dead', linestyle='dashed')\n",
    "ax.plot(t_data_sparse, R_sparse, 'red', alpha=0.5, lw=2, label='Recovered', linestyle='dashed')\n",
    "\n",
    "ax.set_xlabel('Time /days')\n",
    "ax.set_ylabel('Fraction of Population')\n",
    "ax.yaxis.set_tick_params(length=0)\n",
    "ax.xaxis.set_tick_params(length=0)\n",
    "legend = ax.legend()\n",
    "legend.get_frame().set_alpha(0.5)\n",
    "for spine in ('top', 'right', 'bottom', 'left'):\n",
    "    ax.spines[spine].set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network configuration\n",
    "layers = [1, 20, 20, 20, 20, 20, 20, 20, 4]\n",
    "# line up physics mesh to align with baseline\n",
    "timesteps_phys = 500\n",
    "t_phys = np.linspace(0,duration,timesteps_phys)\n",
    "t_phys = np.transpose(t_phys)\n",
    "t_phys = np.reshape(t_phys, (len(t_phys),1))\n",
    "model = PhysicsInformedNN(t_data_sparse, t_phys, Y_sparse, N, layers)\n",
    "\n",
    "#check number of params\n",
    "total_params = sum(p.numel() for p in model.dnn.parameters())\n",
    "print(total_params)\n",
    "\n",
    "model.train(25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot true result comparison to network output\n",
    "t_inter = np.linspace(0,500,500)\n",
    "t_inter = np.transpose(t_inter)\n",
    "t_inter = np.reshape(t_inter, (len(t_inter),1))\n",
    "ttensor = torch.tensor(t_inter).float().to(device)\n",
    "[snet, inet, dnet, rnet] = model.predict(ttensor)\n",
    "plt.plot(t_inter, snet, color = 'red', label = 'susceptible network', linestyle = 'dashed')\n",
    "plt.scatter(t_data_sparse, S_sparse, color = 'red', label = 'susceptible')\n",
    "plt.plot(t_inter, inet, color = 'blue', label = 'infected network', linestyle = 'dashed')\n",
    "plt.scatter(t_data_sparse, I_sparse, color = 'blue', label = 'infected')\n",
    "plt.plot(t_inter, dnet, color = 'black', label = 'dead network', linestyle = 'dashed')\n",
    "plt.scatter(t_data_sparse, D_sparse, color = 'black', label = 'dead')\n",
    "plt.plot(t_inter, rnet, color = 'green', label = 'recovered network', linestyle = 'dashed')\n",
    "plt.scatter(t_data_sparse, R_sparse, color = 'green', label = 'recovered')\n",
    "#True values\n",
    "plt.plot(t_inter, Sc, color = 'red')\n",
    "plt.plot(t_inter, Ic, color = 'blue')\n",
    "plt.plot(t_inter, Dc, color = 'black')\n",
    "plt.plot(t_inter, Rc, color = 'green')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Network recreation of training dataset')\n",
    "plt.xlabel('Time [days]')\n",
    "plt.ylabel('Fraction of population')\n",
    "plt.show()\n",
    "\n",
    "#Plot Noisy Loss (ADAM)\n",
    "plt.plot(model.losses[0:],color = 'black', label = \"Total\")\n",
    "plt.plot(model.residlosses[0:],color = 'green', label = \"Compartment residuals\")\n",
    "plt.plot(model.datalosses[0:],color = 'orange', label = \"Data\")\n",
    "plt.plot(model.Nlosses[0:], color = 'blue', label = \"Population residual\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Loss trends under ADAM optimization\")\n",
    "plt.xlabel('Epochs (in hundreds)')\n",
    "plt.ylabel('Loss [base 10 log]')\n",
    "plt.show()\n",
    "# Print final loss components\n",
    "print('Adam final losses')\n",
    "print('Loss:  %.4e, Residuals loss: %.4e, Data loss: %.4e, Pop Conservation loss: %.4e' % \n",
    "        (\n",
    "            model.losses[-1],\n",
    "            model.residlosses[-1],\n",
    "            model.datalosses[-1],\n",
    "            model.Nlosses[-1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "print('Estimated beta value')\n",
    "print(model.beta.item())\n",
    "\n",
    "print('Estimated gamma value')\n",
    "print(model.gamma.item())\n",
    "\n",
    "print('Estimated mu value')\n",
    "print(model.mu.item())\n",
    "\n",
    "# plot the learned SIRD parameters vs true SIRD parameters\n",
    "plt.plot(model.betas[0:], color = 'teal', label =\"beta\")\n",
    "plt.plot(model.gammas[0:], color = 'red', label=\"gamma\")\n",
    "plt.plot(model.mus[0:], color = 'green', label=\"mu\")\n",
    "plt.axhline(y = 0.191, color = 'teal', linestyle = 'dashed', label =\"betagoal\")\n",
    "plt.axhline(y = 0.05, color = 'red', linestyle = 'dashed', label =\"gammagoal\")\n",
    "plt.axhline(y = 0.0294, color = 'green', linestyle = 'dashed', label =\"mugoal\")\n",
    "plt.legend()\n",
    "plt.title('Change in Parameter estimation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Parameter value')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
